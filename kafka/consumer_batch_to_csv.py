from kafka import KafkaConsumer
import pandas as pd
import json
import os
import uuid
from datetime import datetime

TOPIC_NAME = "sales_topic"
BOOTSTRAP_SERVER = "localhost:9092"
BATCH_SIZE = 100

BASE_DIR = os.path.dirname(os.path.abspath(os.path.abspath(__file__)))
DATA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data'))

consumer = KafkaConsumer(
    TOPIC_NAME,
    bootstrap_servers = BOOTSTRAP_SERVER,
    auto_offset_reset = 'earliest', # consumer ì¼œê¸° ì „ì— ë³´ë‚¸ ë©”ì‹œì§€ë„ ì „ë¶€ ë°›ê¸°
    enable_auto_commit = True,
    group_id = 'batch-csv-consumer',
    value_deserializer = lambda x: json.loads(x.decode('utf-8'))
)

records = []


print("ğŸ“¥ Kafka Consumer Started")
print("ğŸ“¡ Listening for messages...\n")

for message in consumer:
    records.append(message.value)

    if len(records) == BATCH_SIZE:
        df = pd.DataFrame(records)

        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        unique_id = uuid.uuid4().hex[:6]
        filename = f"batch_{timestamp}_{unique_id}.csv"

        # íŒŒì¼ ê²½ë¡œ
        file_path = os.path.join(DATA_DIR, filename)
        # íŒŒì¼ ì €ì¥
        df.to_csv(file_path, index = False)

        print(f"ğŸ’¾ Batch: {filename} â†’ âœ… Saved {len(records)} records to: {file_path}")

        # ë‹¤ìŒ ë°°ì¹˜ ì¤€ë¹„
        records = []